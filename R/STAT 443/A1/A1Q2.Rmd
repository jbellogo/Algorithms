---
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/mnt/storage/home/jbellogo/R/443/")
data <- read.csv("Q2.csv")
```

# a)
```{r}
# Design matrix
X.raw <- model.matrix(~poly(t, 6, raw = TRUE), data)
# Correlation matrix
X.raw.cor <- cor(X.raw)
X.raw.cor
pairs(X.raw)
```

**Comments**: 

* In X.Cor we see all correlation values among the columns are close to 1, so they are close to linear. 

* To interpret the scatterplots, index them by rows and columns according to the powers $p$ of $t^p$ in the design matrix for all $t\in[0,1,2,3,4,5,6]$. Note the plots are somewhat symmetrical about the diagonal since reflected plots $(t^i, t^j)$ and $(t^j, t^i)$ represent the same data just with inverted axes   

* The observation on the correlation matrix above is supported by the scatterplots as most of them seem to follow linear trends. With more clear linear relationships showing for higher powers, ie. the plot for $(t^5,t^6)$ looks like a straight line. The plots between smaller powers still follow clear patterns but not as linear as for higher powers ie, $t^1, t^6$ don't seem linear but they do have a non-random relationships. 

* The plots between the ones columnd and other colums $(1, t^i)$ show vertical and horizontal lines which means the ones columns is independent (NaN correlations as well)

* Across the diagonal, correlations are one.

\newpage

# b)
```{r}
# Design matrix
X.ortho <- model.matrix(~poly(t, 6), data)
# Correlation matrix
X.ortho.cor <- cor(X.ortho)
X.ortho.cor
pairs(X.ortho)
```

**Comments:**

* The plots show no linear relationship between any two columnds of X.ortho. There are patterns between them but some are polynomial and some of them are not even functions.

* The correlation matrix has non-diagonal values in the order of $10^{-17}$ which supports the observation that no two columns of the design matrix are linearly dependent.

\newpage


# c) 
Orthogonal polynomials definitely address concerns about multicolinearity which justifies inferences  and also reduces the length of confidence intervals. 

On the other hand, regular polynomials have coefficients that can be interpreted in terms of the explanatory variates in meaningful ways (ie, $\beta_i$ represents an increase in one unit for variable $x_i$ when all else is kept constant), whereas the coefficients of orthogonal polynomials can not be easily interpreted. 

For fitting, I would prefer regular polynomials to keep the interpretability of their coefficients. For prediction, we should use orthogonal polynomials. 

\newpage


# d) 
```{r}
Yt <- data$Yt
t <- data$t
lm_d <- lm(Yt ~ poly(t, 6, raw = TRUE))
summary(lm_d)
```
**Comment:**

* The $p-$values for coefficients of degrees greated than 3 are large and thus insignificant. So there is no evidence here to reject the null hypotheses: $H_{0,j}:\beta_j=0, \forall j\in[3,4,5,6]$. 

* The $p-$value of the $F$ statistic is very small and thus highly significant. It says there is strong evidence to reject the null hypothesis that our model is better than the intercept-only model.

* Both of the above observations are contradictory and they suggest there is multicolinearity between some of the explanatory variables $\{1,t,t^2,t^3,t^4,t^5,t^6\}$ just as we suspected from the scatterplots in part (a).


\newpage

# e) 
```{r}
lm_e <- lm(Yt ~ poly(t, 6, raw = FALSE))
summary(lm_e)
```

**Comment:**

* The $p-$values for coefficients of degrees $\{2,4,6\}$ are large and thus insignificant. So there is no evidence here to reject the null hypotheses: $H_{0,j}:\beta_j=0, \forall j\in[2,4,6]$. A next step in trying to find a good model would be to drop these powers. 

* The $p-$value of the $F$ statistic is very small and thus highly significant. It says there is strong evidence to reject the null hypothesis that our model is better than the intercept-only model.

\newpage

# f)
```{r}
res_d <- lm_d$residuals
res_e <- lm_e$residuals # orthogonal
lm_res<-lm(res_d ~res_e)
lm_res$coefficients
head(lm_res$residuals)

fit_e <-lm_e$fitted.values # orthogonal
fit_d <-lm_d$fitted.values
lm_fit<-lm(fit_d ~fit_e)
lm_fit$coefficients
head(lm_fit$residuals)


plot(res_e, res_d, 
     xlab = "Orthogonal polynomials fit residuals (e)",
     ylab = "Regular polynomials fit residuals (d)")
abline(lm_res)
plot(fit_e, fit_d,
     xlab = "Orthogonal polynomials fit estimates (e)",
     ylab = "Regular polynomials fit estimates (d)")
abline(lm_fit)

```

**Observations:**

* The residuals for both models follow a near-perfect linear relationship. The slope coefficient of linear model for residuals (lm_res) is virtually 1, there is however a very small intercept between the two, suggesting that residuals for the orthogonal polynomials model in part $e)$ are slighly larger than those for the regular polynomials model in part $e$. The relationship is not perfectly linear as the residuals of lm_res are not zero. 

* The fitted values for both models follow a near-perfect linear relationship. The slope coefficient of linear model for fits (lm_fit) is virtually 1, there is however a very small intercept between the two, suggesting that the fitted values for the orthogonal polynomials model in part $e)$ are slighly larger than those for the regular polynomials model in part $e$. The relationship is not perfectly linear as the residuals of lm_fit are not zero. 



